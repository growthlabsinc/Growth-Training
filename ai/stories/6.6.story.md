# Story 6.6: AI Coach Prompt Engineering & Response Quality Optimization

**Status:** In Progress

## Goal & Context

**User Story:** As a User, I want the AI Coach to provide high-quality, accurate, and helpful responses to my questions about Growth Methods and app functionality, ensuring the information provided is reliable and easy to understand.

**Context:**
This story is the sixth step in Epic 6, focused on enhancing the AI Growth Coach feature. Now that we have ingested content into the knowledge base (from Story 6.5), we need to optimize the prompt engineering for the AI Coach to ensure it delivers accurate, relevant, and concise responses. This involves creating prompt templates that effectively instruct the model on how to respond to different types of user queries, and implementing techniques to improve response quality and reduce potential inaccuracies.

## Detailed Requirements

- Design and implement optimized prompt templates for different query categories (Growth Methods questions, app navigation questions, general wellness questions, etc.).
- Implement proper RAG (Retrieval Augmented Generation) response formatting to clearly indicate when information is coming from our knowledge base versus general model knowledge.
- Add specific instructions in the system prompt to ensure the AI Coach correctly handles out-of-scope questions (as defined in Story 6.4).
- Implement techniques to improve response quality, such as:
  - Clear and specific instructions in system prompts
  - Few-shot examples for common query types
  - Proper context structuring for RAG results
  - Response format standardization
- Create a documentation file `docs/ai-coach-prompt-templates.md` that catalogs all prompt templates and their intended use cases.
- Implement a basic prompt evaluation framework to test and iterate on prompt designs.

## Acceptance Criteria (ACs)

- AC1: Optimized prompt templates are created for at least 5 different query categories.
- AC2: The AI Coach consistently produces responses that correctly reference information from the knowledge base.
- AC3: The AI Coach responses follow a standard format with appropriate sections (e.g., direct answer, supporting details, suggested follow-ups).
- AC4: Edge case queries (very technical questions, out-of-scope questions) are handled appropriately according to the guidelines established in Story 6.4.
- AC5: The AI Coach response quality meets or exceeds a defined quality threshold based on evaluation metrics.
- AC6: Documentation of all prompt templates and design decisions is created in `docs/ai-coach-prompt-templates.md`.

## Technical Implementation Context

**Guidance:**
Developer agent is expected to follow project standards in `docs/coding-standards.md` and understand the project structure in `docs/templates/project-structure.md`. Only story-specific details are included below.

- **Relevant Files:**
  - Files to Create:
    - `scripts/vertex-ai-prompt-evaluation.js` - For testing prompt variations
    - `docs/ai-coach-prompt-templates.md` - Documentation of prompt templates
    - `Growth/Features/AICoach/Services/PromptTemplateService.swift` - Service to manage prompt templates
  - Files to Modify:
    - `Growth/Features/AICoach/Services/AICoachService.swift` - To integrate optimized prompts
    - `Growth/Features/AICoach/ViewModels/CoachChatViewModel.swift` - To connect with updated service
    - `docs/ai-coach-knowledge-base.md` - Update with prompt engineering details

- **Key Technologies:**
  - Vertex AI Prompt Design Best Practices
  - Retrieval Augmented Generation (RAG) with Google's Vertex AI
  - Swift for integrating prompt templates in the app

- **Implementation Notes:**
  - **Prompt Structure:**
    - System prompts should clearly define the AI Coach's role, capabilities, and limitations
    - Include few-shot examples for common query types
    - Clearly instruct on how to format responses that include retrieved information
    - Implement techniques to avoid hallucinations and inaccuracies
  
  - **RAG Implementation:**
    - Structure context insertion to make it clear which parts of the response are coming from our knowledge base
    - Include source attribution when appropriate (e.g., "Based on Growth Method X...")
    - Implement strategies to handle cases when relevant information is not found in the knowledge base
  
  - **Evaluation Framework:**
    - Create a simple framework to test prompts against a set of sample queries
    - Define metrics for response quality (accuracy, relevance, conciseness, helpfulness)
    - Implement A/B testing for different prompt variations
    
  - **Swift Integration:**
    - Design the `PromptTemplateService` to be flexible and maintainable
    - Implement a mechanism to select the appropriate prompt template based on query type
    - Ensure proper error handling when building prompts or processing responses

- **API Usage:**
  - Document all Vertex AI API calls in the code with clear comments
  - Implement proper error handling for API failures
  - Use environment variables for API configurations

- **Documentation Requirements:**
  - Document each prompt template with its purpose, structure, and example
  - Include rationale for design decisions
  - Document the evaluation framework and results
  - Provide guidance for future prompt optimization

## Tasks / Subtasks

- [ ] Research prompt engineering best practices for RAG systems
- [ ] Analyze existing sample queries and categorize them into query types
- [ ] Design and create base system prompt that defines the AI Coach's role and limitations
- [ ] Create specific prompt templates for different query categories:
  - [ ] Growth Method guidance queries
  - [ ] App navigation queries
  - [ ] Progress tracking queries
  - [ ] General wellness questions (within scope)
  - [ ] Out-of-scope questions handling
- [ ] Create the Swift `PromptTemplateService` implementation
  - [ ] Design the prompt template data structure
  - [ ] Implement template selection logic
  - [ ] Connect with AICoachService
- [ ] Modify `AICoachService.swift` to incorporate the new prompt templates
  - [ ] Update the query processing logic
  - [ ] Implement RAG context formatting
  - [ ] Add response post-processing if needed
- [ ] Create prompt evaluation framework in `scripts/vertex-ai-prompt-evaluation.js`
  - [ ] Define evaluation metrics
  - [ ] Create test cases for different query types
  - [ ] Implement scoring functionality
- [ ] Run evaluations and iterate on prompt designs
  - [ ] Test with sample queries
  - [ ] Analyze failure cases
  - [ ] Refine prompts based on results
- [ ] Create comprehensive documentation in `docs/ai-coach-prompt-templates.md`
  - [ ] Document all prompt templates
  - [ ] Include examples of expected inputs and outputs
  - [ ] Document evaluation results and iterations
- [ ] Update integration with CoachChatViewModel
- [ ] Perform final testing and validation against acceptance criteria

## Testing Requirements

**Guidance:**
Verify implementation against the ACs using the following tests. Follow general testing approach in `docs/testing-strategy.md`.

- **Functional Testing:**
  - Test that the PromptTemplateService correctly selects appropriate templates for different query types
  - Test that the AICoachService properly integrates the templates with user queries
  - Test that the response handling correctly processes AI responses
  - Test error handling for edge cases

- **Prompt Quality Testing:**
  - Test prompts with a diverse set of sample queries
  - Evaluate response quality against defined metrics
  - Test with edge case queries to ensure proper handling
  - Test with knowledge base content to ensure proper retrieval and attribution

- **Integration Testing:**
  - Test the end-to-end flow from user query to displayed response
  - Verify correct integration with the knowledge base content ingested in Story 6.5
  - Test performance and response times

- **Documentation Testing:**
  - Verify that `docs/ai-coach-prompt-templates.md` accurately documents all implemented templates
  - Ensure documentation is clear and provides sufficient guidance for future development

## Story Wrap Up (Agent Populates After Execution)

- **Agent Model Used:** `<Agent Model Name/Version>`
- **Completion Notes:** {Any notes about implementation choices, difficulties, or follow-up needed}
- **Change Log:**
  - Initial Draft 